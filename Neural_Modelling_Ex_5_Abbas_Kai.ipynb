{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Modelling Exercise 5\n",
    "Submitted by **Mohammed Abbas Ansari** and **Kai Rothe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maze\n",
    "maze = np.zeros((9, 13))\n",
    "\n",
    "# place walls\n",
    "maze[2, 6:10] = 1\n",
    "maze[-3, 6:10] = 1\n",
    "maze[2:-3, 6] = 1\n",
    "\n",
    "# define start\n",
    "start = np.array([5, 7])\n",
    "\n",
    "# define goal (we abuse function scoping a bit here, later we will change the goal, which will automatically change the goal in our actor critic as well)\n",
    "goal = np.array([1, 1])\n",
    "goal_state = goal[0]*maze.shape[1] + goal[1]\n",
    "goal_value = 10\n",
    "\n",
    "def plot_maze(maze, start = start, goal = goal):\n",
    "    plt.imshow(maze, cmap='binary')\n",
    "\n",
    "    # draw thin grid\n",
    "    for i in range(maze.shape[0]):\n",
    "        plt.plot([-0.5, maze.shape[1]-0.5], [i-0.5, i-0.5], c='gray', lw=0.5)\n",
    "    for i in range(maze.shape[1]):\n",
    "        plt.plot([i-0.5, i-0.5], [-0.5, maze.shape[0]-0.5], c='gray', lw=0.5)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(start[1], start[0], marker='*', color='blue', s=100)\n",
    "    plt.scatter(goal[1], goal[0], marker='*', color='red', s=100)\n",
    "\n",
    "def plot_path(maze, path):\n",
    "    # plot a maze and a path in it\n",
    "    plt.imshow(maze, cmap='binary')\n",
    "\n",
    "    # draw thin grid\n",
    "    for i in range(maze.shape[0]):\n",
    "        plt.plot([-0.5, maze.shape[1]-0.5], [i-0.5, i-0.5], c='gray', lw=0.5)\n",
    "    for i in range(maze.shape[1]):\n",
    "        plt.plot([i-0.5, i-0.5], [-0.5, maze.shape[0]-0.5], c='gray', lw=0.5)\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    path = np.array(path)\n",
    "    plt.plot(path[:, 1], path[:, 0], c='green', lw=3)\n",
    "    plt.scatter(path[0, 1], path[0, 0], marker='*', color='blue', s=100)\n",
    "    plt.scatter(path[-1, 1], path[-1, 0], marker='*', color='red', s=100)\n",
    "\n",
    "plt.title('Blue Star: Start, Red Star: Goal')\n",
    "plot_maze(maze)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Actor-Critic function\n",
    "\n",
    "program an actor critic algorithm to navigate the maze, using a table of action propensities M with softmax action selection as actor, and a learned state-value function as critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the actor-critic algorithm to learn to navigate the maze\n",
    "\n",
    "def state_to_position(state):\n",
    "    return np.array([state // maze.shape[1], state % maze.shape[1]])\n",
    "\n",
    "def position_to_state(position):\n",
    "    return position[..., 0] * maze.shape[1] + position[..., 1]\n",
    "\n",
    "def invalid_position(position):\n",
    "    return not (position[..., 0] >= 0 and position[..., 0] < maze.shape[0] and position[..., 1] >= 0 and position[..., 1] < maze.shape[1] and maze[position[..., 0], position[..., 1]] == 0)\n",
    "\n",
    "def softmax_policy(M, invalid_mask):\n",
    "    # mask out invalid actions\n",
    "    exp_x = np.exp(M - np.max(M))\n",
    "    exp_x[invalid_mask] = 0.\n",
    "    return exp_x / (exp_x.sum() + 1e-8)\n",
    "\n",
    "def normal_start():\n",
    "    # suggested encoding of 2D location onto states\n",
    "    state = start[0]*maze.shape[1] + start[1]\n",
    "    return state\n",
    "\n",
    "def actor_critic(state_representation, n_steps, alpha, gamma, n_episodes, update_sr=False, start_func=normal_start, v_init=0, gamma_sr = 0.8):\n",
    "    # state_representation is a matrix of size n_states by n_states, giving us the representation for each, which is either a 1-hot vector\n",
    "    # # (so e.g. state_representation[15] is a vector of size n_states which is 0 everwhere, except 1 at index 15), or the SR for each state\n",
    "    # n_steps is the number of actions in each episode before it gets cut off, an episode also ends when the agent reaches the goal\n",
    "    # alpha and gamma are the learning rate and discount factor respectively\n",
    "    # n_episodes is the number of episodes to train the agent\n",
    "    # update_sr is for exercise part 3, when you want to update the SR after each episode\n",
    "    # start_func allows you to specify a different starting state, if desired\n",
    "\n",
    "    # initialize M-table\n",
    "    n_states = state_representation.shape[0]\n",
    "    moves = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n",
    "    M = np.zeros((n_states, len(moves)))\n",
    "\n",
    "    # initialize state-value function\n",
    "    V_weights = np.full(n_states, v_init, dtype=np.float64) \n",
    "\n",
    "    earned_rewards = np.zeros(n_episodes)\n",
    "    final_goal_path = None\n",
    "\n",
    "    # iterate over episodes\n",
    "    for episode in range(n_episodes):\n",
    "        # initializations\n",
    "        start_state = start_func()\n",
    "        state_trajectory = []\n",
    "        position_trajectory = []\n",
    "        state_trajectory.append(start_state)\n",
    "        position_trajectory.append(state_to_position(start_state))\n",
    "\n",
    "        # go until goal is reached\n",
    "        for t in range(n_steps):\n",
    "            # pick a valid action\n",
    "            state = state_trajectory[-1]\n",
    "            position = position_trajectory[-1]\n",
    "            next_positions = position + moves\n",
    "            invalid_mask = np.array([invalid_position(p) for p in next_positions])\n",
    "            probability = softmax_policy(M[state, :], invalid_mask)\n",
    "            action = np.random.choice(len(moves), p=probability)\n",
    "            next_position = next_positions[action]\n",
    "            \n",
    "            # store new state\n",
    "            position_trajectory.append(next_position)\n",
    "            new_state = position_to_state(next_position)\n",
    "            state_trajectory.append(new_state)\n",
    "\n",
    "            # critic update\n",
    "            delta = gamma * V_weights[new_state] - V_weights[state] # + reward = 0\n",
    "            V_weights += alpha * delta * state_representation[state] # dV(s, w)/dw = X(s)\n",
    "            \n",
    "            # actor update\n",
    "            log_gradient = - probability # dPi_m(a'|s)/dm = Pi_m(a'|s)\n",
    "            log_gradient[action] += 1 # dPi_m(a|s)/dm = (1 - Pi_m(a|s))\n",
    "            M[state, :] += alpha * delta * gamma**t * log_gradient \n",
    "\n",
    "            if update_sr:\n",
    "                # update state representation \n",
    "                delta_sr = gamma_sr * state_representation[new_state] - state_representation[state]\n",
    "                delta_sr[state] += 1 \n",
    "                state_representation[state] += alpha * delta_sr\n",
    "\n",
    "            # check if goal is reached\n",
    "            if np.all(next_position == goal):\n",
    "                # update values of last state (because break) without taking an action\n",
    "                delta = goal_value - V_weights[new_state] # + gamma * V_weights[state_after_goal] = 0\n",
    "                V_weights += alpha * delta * state_representation[new_state]\n",
    "\n",
    "                if update_sr:\n",
    "                    delta_sr = - state_representation[new_state]\n",
    "                    delta_sr[new_state] += 1\n",
    "                    state_representation[new_state] += alpha * delta_sr\n",
    "\n",
    "                # update earned rewards\n",
    "                earned_rewards[episode] = gamma**t * goal_value\n",
    "                final_goal_path = position_trajectory\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "    return M, V_weights, earned_rewards, final_goal_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_representation = np.eye(maze.size)\n",
    "\n",
    "n_steps = 300\n",
    "alpha = 0.05\n",
    "gamma = 0.99\n",
    "n_episodes = 1000\n",
    "M, V, earned_rewards, example_goal_path = actor_critic(one_hot_representation, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot state-value function\n",
    "plt.title(\"Value Function\")\n",
    "plot_path(maze, example_goal_path)\n",
    "plt.imshow(V.reshape(maze.shape), cmap='hot', alpha=0.82)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot earned reward\n",
    "shortest_path_length = 16\n",
    "plt.plot(np.arange(1, n_episodes+1), earned_rewards)\n",
    "plt.axhline(goal_value * gamma**(shortest_path_length - 1), color='green', linestyle='--', label='Shortest Path')\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Discounted Reward')\n",
    "plt.title('Earned Rewards per Epsiode')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function shows position / states along the shortest path have higher values (towards white) than other states (towards red / black). There is an ascending gradient of values along the shortest path due to discounting of future value. Since the goal is absorbing, the goal state (or a state before when rewarding transitions instead of states) shows the highest value.\n",
    "\n",
    "The learning curves shows the actor-critic converges to the optimal solution (earned reward for shortest path from start to goal of 16 steps, smaller than the goal value of 10 due to the discounting) after around 700 epsiodes, with an example path shown in green above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Improvement with SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(maze):\n",
    "    # for a given maze, compute the transition matrix from any state to any other state under a random walk policy\n",
    "    # (you will need to think of a good way to map any 2D grid coordinates onto a single number for this)\n",
    "\n",
    "    # create a matrix over all state pairs\n",
    "    transitions = np.zeros((maze.size, maze.size))\n",
    "\n",
    "    # iterate over all states, filling in the transition probabilities to all other states on the next step (only one step into the future)\n",
    "    for i in range(maze.shape[0]):\n",
    "        for j in range(maze.shape[1]):\n",
    "            # check if state is valid\n",
    "            if maze[i, j] == 0:\n",
    "                # iterate over all possible moves\n",
    "                for move in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "                    new_i, new_j = i + move[0], j + move[1]\n",
    "                    # check if new state is valid\n",
    "                    if new_i >= 0 and new_i < maze.shape[0] and new_j >= 0 and new_j < maze.shape[1] and maze[new_i, new_j] == 0:\n",
    "                        transitions[i*maze.shape[1] + j, new_i*maze.shape[1] + new_j] = 1\n",
    "    \n",
    "    # normalize transitions\n",
    "    transitions /= transitions.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # remove NaNs\n",
    "    transitions[np.isnan(transitions)] = 0\n",
    "\n",
    "    return transitions\n",
    "\n",
    "def analytical_sr(transitions, gamma):\n",
    "    return np.linalg.inv(np.eye(transitions.shape[0]) - gamma * transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SR for all states, based on the transition matrix\n",
    "# note that we use a lower discounting here, to keep the SR more local\n",
    "transitions = compute_transition_matrix(maze)\n",
    "SR = analytical_sr(transitions, 0.8)\n",
    "\n",
    "M_SR, V_SR, earned_rewards_SR, example_goal_path_SR = actor_critic(SR, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot state-value function\n",
    "plt.title(\"Value Function\")\n",
    "plot_path(maze, example_goal_path_SR)\n",
    "plt.imshow(V_SR.reshape(maze.shape), cmap='hot', alpha=0.82)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot earned reward\n",
    "shortest_path_length = 16\n",
    "plt.plot(np.arange(1, n_episodes+1), earned_rewards_SR)\n",
    "plt.axhline(goal_value * gamma**(shortest_path_length - 1), color='green', linestyle='--', label='Shortest Path')\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Discounted Reward')\n",
    "plt.title('Earned Rewards per Epsiode')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve shows faster learning, i.e. convergence to the optimal solution, for using the successor representation compared to using the one-hot representation from part 1. The value function shows why: after finding the goal once, due to the successor representation, the critic can correctly learn values of never visited states after propagating the goal value back, leading to higher values even of seldomly visited states compared to part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Re-learn SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states = np.arange(maze.size)[np.any(transitions, axis = 0)] # valid states == states that have at least one transition to another state\n",
    "\n",
    "def random_start():\n",
    "    # define yourself a function to return a random (non-wall) starting state to pass into the actor_critic function\n",
    "    return np.random.choice(valid_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = np.array([1, 1])\n",
    "SR_learned = analytical_sr(compute_transition_matrix(maze), 0.8).T\n",
    "M_rl, V_rl, earned_rewards_rl, example_goal_path_rl = actor_critic(SR_learned, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes, update_sr=True, start_func=random_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the SR of some states after this learning, also anything else you want\n",
    "# lt.matshow(SR_learned, cmap='hot')\n",
    "for i in range(3):\n",
    "    random_state = random_start()\n",
    "    plt.title(\"State Representation Before Learning (State {})\".format(random_state))\n",
    "    plot_maze(maze, start = state_to_position(random_state))\n",
    "    plt.imshow(SR[random_state].reshape(maze.shape), cmap='hot', alpha=0.83)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"State Representation After Learning (State {})\".format(random_state))\n",
    "    plot_maze(maze, start = state_to_position(random_state))\n",
    "    plt.imshow(SR_learned[random_state].reshape(maze.shape), cmap='hot', alpha=0.83)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the successor representation of a random walk, the learned successor repressentation shows a higher occupation of states closer to the goal / along the path to the goal, since those states are more likely to be visited after learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot state-value function\n",
    "plt.title(\"Value Function\")\n",
    "plot_path(maze, example_goal_path_rl)\n",
    "plt.imshow(V_rl.reshape(maze.shape), cmap='hot', alpha=0.83)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot earned reward\n",
    "plt.plot(np.arange(1, n_episodes+1), earned_rewards_rl)\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Discounted Reward')\n",
    "plt.title('Earned Rewards per Epsiode')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: How does re-learned SR affect future policy changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = np.array([5, 5])\n",
    "goal_state = position_to_state(goal)\n",
    "\n",
    "n_runs = 40\n",
    "n_episodes = 1000\n",
    "earned_rewards_per_run_original = np.zeros((n_runs, n_episodes))\n",
    "earned_rewards_per_run_relearned = np.zeros((n_runs, n_episodes))\n",
    "\n",
    "for i in tqdm(range(n_runs), desc=\"runs\"):\n",
    "    # run with random walk SR\n",
    "    M, V, earned_rewards_original, _ = actor_critic(SR, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes)\n",
    "    earned_rewards_per_run_original[i, :] = earned_rewards_original\n",
    "\n",
    "    # run with updated SR \n",
    "    M, V, earned_rewards_relearned, _ = actor_critic(SR_learned, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes)\n",
    "    earned_rewards_per_run_relearned[i, :] = earned_rewards_relearned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance averages of the two types of learners\n",
    "earned_rewards_original_mean = earned_rewards_per_run_original.mean(axis=0)\n",
    "earned_rewards_relearned_mean = earned_rewards_per_run_relearned.mean(axis=0)\n",
    "\n",
    "plt.title('Earned Reward for Original vs. Re-Learned SR if Goal = (5, 5)')\n",
    "plt.plot(np.arange(1, n_episodes+1), earned_rewards_original_mean, label='Random Walk Policy')\n",
    "plt.plot(np.arange(1, n_episodes+1), earned_rewards_relearned_mean, label='Tuned towards (1, 1)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Discounted Reward (20 Runs)')\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve for the re-learned successor representation (tuned towards not the goal) shows a lower slope and slower convergence to the optimal solution compared to the learning curve of the original successor representation (of a random walk policy). This can be explained by the fact that the tuned SR introduces an initial bias of going favoring a wrong goal position, while the original SR introduces no bias. Thus in the first case, the actor only later finds and learsn to move towards the new goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Value Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset goal\n",
    "goals = np.array([1, 1])\n",
    "goal_state = position_to_state(goal)\n",
    "\n",
    "# run some learners with different value weight w initializations\n",
    "v_inits = [0, 0.1, 1, 10, 45, 90]\n",
    "n_runs = 12\n",
    "n_episodes = 1000\n",
    "\n",
    "earned_rewards_per_v_init_and_run_sr = np.zeros((len(v_inits), n_runs, n_episodes))\n",
    "earned_rewards_per_v_init_and_run_one_hot = np.zeros_like(earned_rewards_per_v_init_and_run_sr)\n",
    "\n",
    "for i, v_init in enumerate(tqdm(v_inits, desc=\"v_inits\")):\n",
    "    for run in tqdm(range(n_runs), desc=\"runs\", leave=True):\n",
    "        M, V, earned_rewards_sr, _ = actor_critic(SR, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes, v_init=v_init)\n",
    "        earned_rewards_per_v_init_and_run_sr[i, run, :] = earned_rewards_sr\n",
    "        \n",
    "        M, V, earned_rewards_one_hot, _ = actor_critic(one_hot_representation, n_steps=n_steps, alpha=alpha, gamma=gamma, n_episodes=n_episodes, v_init=v_init)\n",
    "        earned_rewards_per_v_init_and_run_one_hot[i, run, :] = earned_rewards_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting learning curves\n",
    "earned_rewards_per_v_init_and_run_sr_mean = earned_rewards_per_v_init_and_run_sr.mean(axis=1)\n",
    "earned_rewards_per_v_init_and_run_one_hot_mean = earned_rewards_per_v_init_and_run_one_hot.mean(axis=1)\n",
    "\n",
    "# SR plot\n",
    "plt.title('Learning Curves for Successor Representation')\n",
    "for i, v_init in enumerate(v_inits):\n",
    "    plt.plot(np.arange(1, n_episodes+1), earned_rewards_per_v_init_and_run_sr_mean[i, :], label=str(v_init))\n",
    "# plt.plot(np.arange(1, n_episodes+1), earned_rewards_relearned_mean, label='Tuned towards (1, 1)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(f'Average Discounted Reward ({n_runs} Runs)')\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.legend(title=\"Initial Weights\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# one-hot plot\n",
    "plt.title('Learning Curves for One Hot Representation')\n",
    "for i, v_init in enumerate(v_inits):\n",
    "    plt.plot(np.arange(1, n_episodes+1), earned_rewards_per_v_init_and_run_one_hot_mean[i, :], label=str(v_init))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(f'Average Discounted Reward ({n_runs} Runs)')\n",
    "plt.xticks(np.concatenate([np.ones(1), np.arange(100, n_episodes+1, 100)]))\n",
    "plt.legend(title=\"Initial Weights\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all weight initializations, learning with the successor representation is faster than using the one-hot representation, as observed in part 2. High weight initializations > 1 converge more slowly (or not at all, at least after 1000 episodes) to the optimal solution, since with higher initial values it takes more updates (assuming equal occupation and learning rate) to increase the difference between values of different states and thus take optimal states instead of suboptimal states. Initial positive weights smaller equal 1 are observed to lead to faster learning than for zero weight initialization, clearly for the one-hot representation and also in the beginning of learning curves for higher weights than 1, e.g. 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
